{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91b8dc",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange; font-size:48px; text-align:center\">Topic 5: Language Understanding solution with Azure AI Language</h1>\n",
    "\n",
    "Azure AI Language Service offers a plethora of features that allow for a comprehensive understanding of human language. It ensures applications can effectively communicate with users and derive insights from user language patterns.\n",
    "\n",
    "# Categories of Features:\n",
    "Azure AI Language Service classifies its features into two categories:\n",
    "\n",
    "- **1) Pre-configured features** - These are out-of-the-box features that do not require additional training or model labeling. They're ready for use upon resource creation.\n",
    "- **2) Learned features** - These require custom data labeling, model training, and deployment. They are tailored to specific needs based on the input data and training.\n",
    "\n",
    "For in-depth details, always refer to the official Azure AI Language service documentation through following link.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview\n",
    "\n",
    "https://learn.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python#azure-ai-textanalytics-textanalyticsclient-begin-abstract-summary\n",
    "\n",
    "**Note:** This notebook covers only Pre-Configured features. In upcomming module, we will look at the Custom/Learned features.\n",
    "\n",
    "# How it Works:\n",
    "All queries to the Azure AI Language Service follow a specific URL pattern:\n",
    "\n",
    "https://{ENDPOINT}/text/analytics/{VERSION}/{FEATURE}\n",
    "\n",
    "- **{ENDPOINT}:** This represents the endpoint for authenticating your API request, such as myLanguageService.cognitiveservices.azure.com.\n",
    "- **{VERSION}:** The version number of the service, like v3.0 or 2022-05-01.\n",
    "- **{FEATURE}:** This is the specific feature you are trying to access, like keyPhrases for key phrase detection.\n",
    "\n",
    "A JSON body accompanying the POST request defines input documents, tasks, and other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2304fe",
   "metadata": {},
   "source": [
    "# Pre-configured Features\n",
    "## 1. Summarization\n",
    "Summarization condenses large pieces of text into shorter, key sentences that capture the essence of the content.\n",
    "\n",
    "**Example:** If you provide a lengthy review of a book, the summarization feature could return a concise summary, highlighting the main points of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282d22f",
   "metadata": {},
   "source": [
    "### 1.1 Summary Approaches\n",
    "\n",
    "Abstractive and extractive summarization are two different approaches to automatically generating concise and coherent summaries of longer texts, such as articles, documents, or news articles. They differ in how they generate summaries:\n",
    "\n",
    "#### a) **Extractive Summarization**\n",
    "\n",
    "   - Extractive summarization works by selecting and extracting sentences or phrases directly from the input text.\n",
    "   - It identifies the most informative and important sentences or phrases from the original text and includes them in the summary.\n",
    "   - The sentences selected for the summary are typically not modified; they are presented as-is from the original text.\n",
    "   - Extractive summarization methods often use techniques like sentence scoring, ranking, and keyword extraction to determine the most relevant content.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "   - Extractive summaries are often more factually accurate since they use sentences directly from the source text.\n",
    "   - They are relatively easier to implement and evaluate.\n",
    "**Limitations:**\n",
    "\n",
    "   - Extractive summaries may not always be as coherent or fluent as abstractive summaries since they are composed of individual sentences that may not fit together seamlessly.\n",
    "   - They might not capture the nuances or broader context of the text effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4415d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Credentials from Excel\n",
    "import pandas as pd\n",
    "df_cred = pd.read_excel(\"credentials.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521e5a6",
   "metadata": {},
   "source": [
    "## How to get KEY & ENDPOINT?\n",
    "\n",
    "To create an Azure AI Services multi-service account and obtain the endpoint and key for a Python client app, you can follow these steps:\n",
    "\n",
    "Step 1: Sign in to Azure Portal\n",
    "- If you don't have an Azure account, you'll need to sign up for one. Once you have an account, log in to the Azure Portal.\n",
    "\n",
    "Step 2: Create a New Azure AI Service Account\n",
    "\n",
    "- In the Azure Portal, click on \"Create a resource\" in the left-hand menu.\n",
    "\n",
    "- Search for \"Azure AI\" in the search bar and select \"Azure AI.\"\n",
    "\n",
    "- Click on \"Create\" to start configuring your Azure AI service account.\n",
    "\n",
    "- In the \"Basics\" tab, provide the following information:\n",
    "\n",
    "    - Subscription: Choose your Azure subscription.\n",
    "    - Resource Group: Create a new resource group or select an existing one.\n",
    "    - Region: Choose the region where you want to deploy the AI service.\n",
    "    - Under the \"Configuration\" section, select \"Multi-service.\"\n",
    "\n",
    "- Provide a unique name for your multi-service account.\n",
    "\n",
    "- Leave the rest of the options as their default values or configure them according to your needs.\n",
    "\n",
    "- Review the configuration, and then click \"Review + create.\"\n",
    "\n",
    "- Review the summary, and if everything looks correct, click \"Create\" to create the multi-service account.\n",
    "\n",
    "Step 3: Obtain Endpoint and Key\n",
    "- After the multi-service account is created, follow these steps to obtain the endpoint and key:\n",
    "\n",
    "- Once the deployment is complete, navigate to the multi-service account you just created in the Azure Portal.\n",
    "\n",
    "- In the left-hand menu, under \"Settings,\" click on \"Keys and Endpoint.\"\n",
    "\n",
    "- You will see two keys, usually named \"Key 1\" and \"Key 2.\" Either key can be used for authentication. Copy one of the keys and store it securely.\n",
    "\n",
    "- Below the keys, you will find the \"Endpoint\" URL. Copy the URL; this is the endpoint you will use to access the AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b89461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. The goal is to have pretrained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "endpoint = df_cred[\"endpoint\"][0]\n",
    "key = df_cred[\"key\"][0]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "   endpoint=endpoint,\n",
    "   credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "document = [\n",
    "   \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "   \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "   \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "   \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "   \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "   \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "   \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "   \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "   \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "   \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "   \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "   \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "   \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "   \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "   \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "   \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "]\n",
    "\n",
    "poller = text_analytics_client.begin_extract_summary(document)\n",
    "extract_summary_results = poller.result()\n",
    "for result in extract_summary_results:\n",
    "    if result.kind == \"ExtractiveSummarization\":\n",
    "        print(\"Summary extracted: \\n{}\".format(\n",
    "           \" \".join([sentence.text for sentence in result.sentences]))\n",
    "       )\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "           result.error.code, result.error.message\n",
    "       ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a7c7b",
   "metadata": {},
   "source": [
    "### b) **Abstractive Summarization**\n",
    "\n",
    "   - Abstractive summarization, on the other hand, aims to generate summaries that are not directly copied from the input text but are generated in a more human-like manner.\n",
    "   - It involves understanding the input text, interpreting its meaning, and then generating concise and coherent summaries in natural language.\n",
    "   - Abstractive summarization methods use techniques such as natural language generation (NLG) and may rephrase and restructure sentences to create summaries that are more fluent and coherent.\n",
    "   - They often require more advanced natural language processing and machine learning techniques, including neural networks and language models.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "   - Abstractive summaries can provide more human-like, fluent, and coherent summaries that may capture the essence of the text better.\n",
    "   - They are flexible and can generate summaries even when there are no suitable sentences to extract directly.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "   - Abstractive summarization is a more challenging task, and the quality of abstractive summaries can vary depending on the complexity of the text and the quality of the summarization model.\n",
    "   - There is a risk of generating inaccurate or biased summaries, especially if the model misunderstands the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b4bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "Microsoft has been working to advance AI beyond existing techniques by taking a more holistic, human-centric approach to learning and understanding. The Chief Technology Officer of Azure AI Cognitive Services, who enjoys a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text, audio or visual sensory signals, and multilingual, has created XYZ-code, a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. Over the past five years, Microsoft has achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "endpoint = df_cred[\"endpoint\"][0]\n",
    "key = df_cred[\"key\"][0]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "   endpoint=endpoint,\n",
    "   credential=AzureKeyCredential(key))\n",
    "\n",
    "document = [\n",
    "   \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "   \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "   \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "   \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "   \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "   \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "   \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "   \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "   \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "   \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "   \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "   \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "   \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "   \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "   \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "   \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "]\n",
    "\n",
    "poller = text_analytics_client.begin_abstract_summary(document)\n",
    "abstract_summary_results = poller.result()\n",
    "for result in abstract_summary_results:\n",
    "    if result.kind == \"AbstractiveSummarization\":\n",
    "        print(\"Summaries abstracted:\")\n",
    "        [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "           result.error.code, result.error.message\n",
    "       ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c615c",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER)\n",
    "NER identifies entities such as names of people, places, organizations, and other specifics within a given text.\n",
    "\n",
    "**Example:** From the sentence \"The Eiffel Tower is located in Paris.\", NER would identify \"Eiffel Tower\" as a monument and \"Paris\" as a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718cd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 'Foo Company' has category 'Organization'\n",
      "Entity 'Contoso' has category 'Organization'\n",
      "Entity 'annual' has category 'DateTime'\n",
      "Entity 'founding ceremony' has category 'Event'\n",
      "Entity 'food' has category 'Product'\n",
      "Entity 'Foo Company' has category 'Organization'\n",
      "Entity 'Contoso' has category 'Person'\n",
      "Entity 'food' has category 'Product'\n",
      "Entity 'catering' has category 'Skill'\n",
      "Entity 'Bar Company' has category 'Organization'\n",
      "Entity 'Contoso' has category 'Organization'\n",
      "Entity 'sliders' has category 'Product'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "endpoint = df_cred[\"endpoint\"][0]\n",
    "key = df_cred[\"key\"][0]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "reviews = [\n",
    "   \"\"\"I work for Foo Company, and we hired Contoso for our annual founding ceremony. The food\n",
    "   was amazing and we all can't say enough good words about the quality and the level of service.\"\"\",\n",
    "    \n",
    "   \"\"\"We at the Foo Company re-hired Contoso after all of our past successes with the company.\n",
    "   Though the food was still great, I feel there has been a quality drop since their last time\n",
    "   catering for us. Is anyone else running into the same problem?\"\"\",\n",
    "    \n",
    "   \"\"\"Bar Company is over the moon about the service we received from Contoso, the best sliders ever!!!!\"\"\"\n",
    "]\n",
    "\n",
    "result = text_analytics_client.recognize_entities(reviews)\n",
    "result = [review for review in result if not review.is_error]\n",
    "\n",
    "for idx, review in enumerate(result):\n",
    "    for entity in review.entities:\n",
    "        print(f\"Entity '{entity.text}' has category '{entity.category}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169957b4",
   "metadata": {},
   "source": [
    "## 3. Personally Identifiable Information (PII) Detection\n",
    "In today's digital age, ensuring data privacy and security is paramount. This is particularly true for Personally Identifiable Information (PII) - any information that can be used to identify an individual. This might include names, addresses, social security numbers, email addresses, and more. Given the importance of safeguarding PII, how can you swiftly detect and handle such data in your applications?\n",
    "\n",
    "Azure AI Language Service provides the recognize_pii_entities feature, which is designed precisely for this purpose.\n",
    "\n",
    "#### What is recognize_pii_entities?\n",
    "This feature of the Azure AI Language Service is adept at detecting PII within a given text. For instance, if you supply a document that contains text like \"John's email address is john.doe@example.com\", the service would identify \"John\" and \"john.doe@example.com\" as PII. It does more than just identifying; it categorizes the type of PII it found. In this example, it would categorize \"John\" as a person's name and \"john.doe@example.com\" as an email address.\n",
    "\n",
    "Here's a hypothetical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebe79f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's compare the original document with the documents after redaction. I also want to comb through all of the entities that got redacted\n",
      "Document text: Parker Doe has repaid all of their loans as of 2020-04-25.\n",
      "   Their SSN is 859-98-0987. To contact them, use their phone number\n",
      "   555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68\n",
      "Redacted document text: ********** has repaid all of their loans as of **********.\n",
      "   Their SSN is ***********. To contact them, use their phone number\n",
      "   ************. They are originally from Brazil and have Brazilian CPF number 998.214.865-68\n",
      "...Entity 'Parker Doe' with category 'Person' got redacted\n",
      "...Entity '2020-04-25' with category 'DateTime' got redacted\n",
      "...Entity '859-98-0987' with category 'USSocialSecurityNumber' got redacted\n",
      "...Entity '555-555-5555' with category 'PhoneNumber' got redacted\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "   \"\"\"Parker Doe has repaid all of their loans as of 2020-04-25.\n",
    "   Their SSN is 859-98-0987. To contact them, use their phone number\n",
    "   555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68\"\"\"\n",
    "]\n",
    "\n",
    "result = text_analytics_client.recognize_pii_entities(documents)\n",
    "docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "print(\n",
    "   \"Let's compare the original document with the documents after redaction. \"\n",
    "   \"I also want to comb through all of the entities that got redacted\"\n",
    ")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Document text: {documents[idx]}\")\n",
    "    print(f\"Redacted document text: {doc.redacted_text}\")\n",
    "    for entity in doc.entities:\n",
    "        print(\"...Entity '{}' with category '{}' got redacted\".format(\n",
    "           entity.text, entity.category\n",
    "       ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7c61d",
   "metadata": {},
   "source": [
    "## 4. Key Phrase Extraction\n",
    "It extracts main ideas or themes from a provided text.\n",
    "\n",
    "**Example:** For the statement \"Pollution is adversely affecting marine life.\", the extracted key phrase might be \"adversely affecting marine life\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e072f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key phrases in article #1: D.C. Autumn, beautiful season, clearer view, blue sky, yellow leaves, Washington, DC, trees, city, forests, ground\n",
      "Key phrases in article #2: United States workers, start date, Redmond, WA, past, days, Microsoft, pandemic, end, sight\n",
      "Key phrases in article #3: new coffee shop, Redmond, WA, Employees, Microsoft, campus, workers\n"
     ]
    }
   ],
   "source": [
    "articles = [\n",
    "   \"\"\"\n",
    "   Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall from the trees\n",
    "   in a city chock-full of forests, leaving yellow leaves on the ground and a clearer view of the\n",
    "   blue sky above...\n",
    "   \"\"\",\n",
    "   \"\"\"\n",
    "   Redmond, WA. In the past few days, Microsoft has decided to further postpone the start date of\n",
    "   its United States workers, due to the pandemic that rages with no end in sight...\n",
    "   \"\"\",\n",
    "   \"\"\"\n",
    "   Redmond, WA. Employees at Microsoft can be excited about the new coffee shop that will open on campus\n",
    "   once workers no longer have to work remotely...\n",
    "   \"\"\"\n",
    "]\n",
    "\n",
    "result = text_analytics_client.extract_key_phrases(articles)\n",
    "for idx, doc in enumerate(result):\n",
    "    if not doc.is_error:\n",
    "        print(\"Key phrases in article #{}: {}\".format(\n",
    "           idx + 1,\n",
    "           \", \".join(doc.key_phrases)\n",
    "       ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a5cf3",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "This determines the sentiment or tone behind a text, classifying it as positive, negative, neutral, or mixed.\n",
    "\n",
    "**Example:** \"I love this product!\" would be classified as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb958987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment: positive\n",
      "Document text: I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.\n",
      "   I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.\n",
      "==============================================================================================================\n",
      "Overall sentiment: negative\n",
      "Document text: This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would\n",
      "   not recommend to any divers, even first timers.\n",
      "==============================================================================================================\n",
      "Overall sentiment: positive\n",
      "Document text: This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience\n",
      "==============================================================================================================\n",
      "Overall sentiment: positive\n",
      "Document text: I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right\n",
      "   in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,\n",
      "   I know she'll love it!\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "   \"\"\"I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.\n",
    "   I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.\"\"\",\n",
    "   \"\"\"This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would\n",
    "   not recommend to any divers, even first timers.\"\"\",\n",
    "   \"\"\"This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience\"\"\",\n",
    "   \"\"\"I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right\n",
    "   in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,\n",
    "   I know she'll love it!\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Overall sentiment: {doc.sentiment}\")\n",
    "    print(f\"Document text: {documents[idx]}\")\n",
    "    print(\"=\"*110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5d33f",
   "metadata": {},
   "source": [
    "## 6. Language Detection\n",
    "Detects the language of a given text.\n",
    "\n",
    "**Example:** \"Hola, ¿cómo estás?\" would be identified as Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700fe893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Language</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hated the movie. It was so slow!</td>\n",
       "      <td>English</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J'ai détesté le film. C'était si lent !</td>\n",
       "      <td>French</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ما له فلم څخه کرکه وکړه. دا دومره ورو وه!</td>\n",
       "      <td>Pashto</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مجھے فلم سے نفرت تھی۔ یہ بہت سست تھا!</td>\n",
       "      <td>Urdu</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The movie made it into my top ten favorites. W...</td>\n",
       "      <td>English</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message Language  Confidence\n",
       "0                 I hated the movie. It was so slow!  English        0.98\n",
       "1            J'ai détesté le film. C'était si lent !   French        1.00\n",
       "2          ما له فلم څخه کرکه وکړه. دا دومره ورو وه!   Pashto        1.00\n",
       "3              مجھے فلم سے نفرت تھی۔ یہ بہت سست تھا!     Urdu        1.00\n",
       "4  The movie made it into my top ten favorites. W...  English        0.99"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"I hated the movie. It was so slow!\", \n",
    "             \"J'ai détesté le film. C'était si lent !\", \n",
    "             \"ما له فلم څخه کرکه وکړه. دا دومره ورو وه!\"\n",
    "             , \"مجھے فلم سے نفرت تھی۔ یہ بہت سست تھا!\", \n",
    "             \"The movie made it into my top ten favorites. What a great movie!\"]\n",
    "\n",
    "languages = text_analytics_client.detect_language(documents)\n",
    "\n",
    "language = []\n",
    "confidence = []\n",
    "\n",
    "for res in languages:\n",
    "    language.append(res[\"primary_language\"][\"name\"])\n",
    "    confidence.append(res[\"primary_language\"][\"confidence_score\"])\n",
    "\n",
    "df = pd.DataFrame({\"Message\": documents, \"Language\": language, \"Confidence\": confidence})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b60d48",
   "metadata": {},
   "source": [
    "## Multiple Analysis at Once\n",
    "\n",
    "**begin_analyze_actions** performs multiple analyses over one set of documents in a single request. Currently it is supported using any combination of the following Language APIs in a single request:\n",
    "\n",
    "- Entities Recognition\n",
    "- PII Entities Recognition\n",
    "- Linked Entity Recognition\n",
    "- Key Phrase Extraction\n",
    "- Sentiment Analysis\n",
    "- Extractive Summarization (API version 2022-10-01-preview and newer)\n",
    "- Abstractive Summarization (API version 2022-10-01-preview and newer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32eea16a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document text: We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) and he is super nice, coming out of the kitchen and greeted us all.\n",
      "...Results of Recognize Entities Action:\n",
      "......Entity: Contoso Steakhouse\n",
      ".........Category: Location\n",
      ".........Confidence Score: 0.68\n",
      ".........Offset: 11\n",
      "......Entity: midtown\n",
      ".........Category: Location\n",
      ".........Confidence Score: 0.57\n",
      ".........Offset: 41\n",
      "......Entity: NYC\n",
      ".........Category: Location\n",
      ".........Confidence Score: 0.7\n",
      ".........Offset: 49\n",
      "......Entity: last week\n",
      ".........Category: DateTime\n",
      ".........Confidence Score: 0.8\n",
      ".........Offset: 53\n",
      "......Entity: dinner party\n",
      ".........Category: Event\n",
      ".........Confidence Score: 0.93\n",
      ".........Offset: 69\n",
      "......Entity: chief cook\n",
      ".........Category: PersonType\n",
      ".........Confidence Score: 0.86\n",
      ".........Offset: 166\n",
      "......Entity: owner\n",
      ".........Category: PersonType\n",
      ".........Confidence Score: 0.98\n",
      ".........Offset: 195\n",
      "......Entity: John Doe\n",
      ".........Category: Person\n",
      ".........Confidence Score: 1.0\n",
      ".........Offset: 222\n",
      "......Entity: kitchen\n",
      ".........Category: Location\n",
      ".........Confidence Score: 0.95\n",
      ".........Offset: 272\n",
      "...Results of Recognize PII Entities action:\n",
      "......Entity: Contoso\n",
      ".........Category: Organization\n",
      ".........Confidence Score: 0.58\n",
      "......Entity: last week\n",
      ".........Category: DateTime\n",
      ".........Confidence Score: 0.8\n",
      "......Entity: chief cook\n",
      ".........Category: PersonType\n",
      ".........Confidence Score: 0.64\n",
      "......Entity: owner\n",
      ".........Category: PersonType\n",
      ".........Confidence Score: 0.93\n",
      "......Entity: John Doe\n",
      ".........Category: Person\n",
      ".........Confidence Score: 0.98\n",
      "...Results of Extract Key Phrases action:\n",
      "......Key Phrases: ['Contoso Steakhouse', 'midtown NYC', 'dinner party', 'marvelous food', 'great menu', 'chief cook', 'John Doe', 'spot', 'owner', 'name', 'kitchen']\n",
      "...Results of Recognize Linked Entities action:\n",
      "......Entity name: Steakhouse\n",
      ".........Data source: Wikipedia\n",
      ".........Data source language: en\n",
      ".........Data source entity ID: Steakhouse\n",
      ".........Data source URL: https://en.wikipedia.org/wiki/Steakhouse\n",
      ".........Document matches:\n",
      "............Match text: Steakhouse\n",
      "............Confidence Score: 0.75\n",
      "............Offset: 19\n",
      "............Length: 10\n",
      "......Entity name: New York City\n",
      ".........Data source: Wikipedia\n",
      ".........Data source language: en\n",
      ".........Data source entity ID: New York City\n",
      ".........Data source URL: https://en.wikipedia.org/wiki/New_York_City\n",
      ".........Document matches:\n",
      "............Match text: NYC\n",
      "............Confidence Score: 0.37\n",
      "............Offset: 49\n",
      "............Length: 3\n",
      "......Entity name: John Doe\n",
      ".........Data source: Wikipedia\n",
      ".........Data source language: en\n",
      ".........Data source entity ID: John Doe\n",
      ".........Data source URL: https://en.wikipedia.org/wiki/John_Doe\n",
      ".........Document matches:\n",
      "............Match text: John Doe\n",
      "............Confidence Score: 0.05\n",
      "............Offset: 222\n",
      "............Length: 8\n",
      "...Results of Analyze Sentiment action:\n",
      "......Overall sentiment: positive\n",
      "......Scores: positive=0.97;                 neutral=0.03;                 negative=0.0 \n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Document text: We enjoyed very much dining in the place! The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! The only complaint I have is the food didn't come fast enough. Overall I highly recommend it!\n",
      "...Results of Recognize Entities Action:\n",
      "......Entity: Sirloin steak\n",
      ".........Category: Product\n",
      ".........Confidence Score: 0.85\n",
      ".........Offset: 46\n",
      "......Entity: www.contososteakhouse.com\n",
      ".........Category: URL\n",
      ".........Confidence Score: 0.8\n",
      ".........Offset: 177\n",
      "......Entity: 312-555-0176\n",
      ".........Category: PhoneNumber\n",
      ".........Confidence Score: 0.8\n",
      ".........Offset: 209\n",
      "......Entity: order@contososteakhouse.com\n",
      ".........Category: Email\n",
      ".........Confidence Score: 0.8\n",
      ".........Offset: 239\n",
      "......Entity: food\n",
      ".........Category: Product\n",
      ".........Confidence Score: 0.71\n",
      ".........Offset: 301\n",
      "...Results of Recognize PII Entities action:\n",
      "......Entity: www.contososteakhouse.com\n",
      ".........Category: URL\n",
      ".........Confidence Score: 0.8\n",
      "......Entity: 312-555-0176\n",
      ".........Category: PhoneNumber\n",
      ".........Confidence Score: 0.8\n",
      "......Entity: order@contososteakhouse.com\n",
      ".........Category: Email\n",
      ".........Confidence Score: 0.8\n",
      "......Entity: contososteakhouse\n",
      ".........Category: Organization\n",
      ".........Confidence Score: 0.46\n",
      "...Results of Extract Key Phrases action:\n",
      "......Key Phrases: ['The Sirloin steak', 'online menu', 'dining', 'place', 'order', 'email', 'complaint', 'food']\n",
      "...Results of Recognize Linked Entities action:\n",
      "......Entity name: Sirloin steak\n",
      ".........Data source: Wikipedia\n",
      ".........Data source language: en\n",
      ".........Data source entity ID: Sirloin steak\n",
      ".........Data source URL: https://en.wikipedia.org/wiki/Sirloin_steak\n",
      ".........Document matches:\n",
      "............Match text: Sirloin steak\n",
      "............Confidence Score: 0.69\n",
      "............Offset: 46\n",
      "............Length: 13\n",
      "...Results of Analyze Sentiment action:\n",
      "......Overall sentiment: mixed\n",
      "......Scores: positive=0.72;                 neutral=0.03;                 negative=0.25 \n",
      "\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import (\n",
    "    TextAnalyticsClient,\n",
    "    RecognizeEntitiesAction,\n",
    "    RecognizeLinkedEntitiesAction,\n",
    "    RecognizePiiEntitiesAction,\n",
    "    ExtractKeyPhrasesAction,\n",
    "    AnalyzeSentimentAction,\n",
    ")\n",
    "\n",
    "endpoint = df_cred[\"endpoint\"][0]\n",
    "key = df_cred[\"key\"][0]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '\n",
    "    'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '\n",
    "    'and he is super nice, coming out of the kitchen and greeted us all.'\n",
    "    ,\n",
    "\n",
    "    'We enjoyed very much dining in the place! '\n",
    "    'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '\n",
    "    'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '\n",
    "    'The only complaint I have is the food didn\\'t come fast enough. Overall I highly recommend it!'\n",
    "]\n",
    "\n",
    "poller = text_analytics_client.begin_analyze_actions(\n",
    "    documents,\n",
    "    display_name=\"Sample Text Analysis\",\n",
    "    actions=[\n",
    "        RecognizeEntitiesAction(),\n",
    "        RecognizePiiEntitiesAction(),\n",
    "        ExtractKeyPhrasesAction(),\n",
    "        RecognizeLinkedEntitiesAction(),\n",
    "        AnalyzeSentimentAction(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "document_results = poller.result()\n",
    "for doc, action_results in zip(documents, document_results):\n",
    "    print(f\"\\nDocument text: {doc}\")\n",
    "    for result in action_results:\n",
    "        if result.kind == \"EntityRecognition\":\n",
    "            print(\"...Results of Recognize Entities Action:\")\n",
    "            for entity in result.entities:\n",
    "                print(f\"......Entity: {entity.text}\")\n",
    "                print(f\".........Category: {entity.category}\")\n",
    "                print(f\".........Confidence Score: {entity.confidence_score}\")\n",
    "                print(f\".........Offset: {entity.offset}\")\n",
    "\n",
    "        elif result.kind == \"PiiEntityRecognition\":\n",
    "            print(\"...Results of Recognize PII Entities action:\")\n",
    "            for pii_entity in result.entities:\n",
    "                print(f\"......Entity: {pii_entity.text}\")\n",
    "                print(f\".........Category: {pii_entity.category}\")\n",
    "                print(f\".........Confidence Score: {pii_entity.confidence_score}\")\n",
    "\n",
    "        elif result.kind == \"KeyPhraseExtraction\":\n",
    "            print(\"...Results of Extract Key Phrases action:\")\n",
    "            print(f\"......Key Phrases: {result.key_phrases}\")\n",
    "\n",
    "        elif result.kind == \"EntityLinking\":\n",
    "            print(\"...Results of Recognize Linked Entities action:\")\n",
    "            for linked_entity in result.entities:\n",
    "                print(f\"......Entity name: {linked_entity.name}\")\n",
    "                print(f\".........Data source: {linked_entity.data_source}\")\n",
    "                print(f\".........Data source language: {linked_entity.language}\")\n",
    "                print(\n",
    "                    f\".........Data source entity ID: {linked_entity.data_source_entity_id}\"\n",
    "                )\n",
    "                print(f\".........Data source URL: {linked_entity.url}\")\n",
    "                print(\".........Document matches:\")\n",
    "                for match in linked_entity.matches:\n",
    "                    print(f\"............Match text: {match.text}\")\n",
    "                    print(f\"............Confidence Score: {match.confidence_score}\")\n",
    "                    print(f\"............Offset: {match.offset}\")\n",
    "                    print(f\"............Length: {match.length}\")\n",
    "\n",
    "        elif result.kind == \"SentimentAnalysis\":\n",
    "            print(\"...Results of Analyze Sentiment action:\")\n",
    "            print(f\"......Overall sentiment: {result.sentiment}\")\n",
    "            print(\n",
    "                f\"......Scores: positive={result.confidence_scores.positive}; \\\n",
    "                neutral={result.confidence_scores.neutral}; \\\n",
    "                negative={result.confidence_scores.negative} \\n\"\n",
    "            )\n",
    "\n",
    "        elif result.is_error is True:\n",
    "            print(\n",
    "                f\"...Is an error with code '{result.error.code}' and message '{result.error.message}'\"\n",
    "            )\n",
    "\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc31fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
